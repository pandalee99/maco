# MACO

<p align="center">
  <b>Multi-GPU Async Communication Optimizer</b><br>
  <i>SM-level Task Scheduling for Compute-Communication Overlap</i>
</p>

<p align="center">
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python 3.8+"></a>
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/pytorch-2.0+-orange.svg" alt="PyTorch 2.0+"></a>
  <a href="https://developer.nvidia.com/cuda-toolkit"><img src="https://img.shields.io/badge/cuda-11.0+-green.svg" alt="CUDA 11.0+"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-yellow.svg" alt="License: MIT"></a>
</p>

---

MACO æ˜¯ä¸€ä¸ª PyTorch ä¼˜åŒ–æ¡†æ¶ï¼Œé€šè¿‡ **SM çº§åˆ«ä»»åŠ¡è°ƒåº¦** å®ç°è®¡ç®—ä¸é€šä¿¡çš„é«˜æ•ˆé‡å ã€‚æ ¸å¿ƒæŠ€æœ¯å€Ÿé‰´ [Mirage](https://github.com/mirage-project/mirage)ï¼Œ**æ— éœ€ NVSHMEM**ï¼Œä»…ä¾èµ–æ ‡å‡† PyTorch + CUDAã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | æè¿° | æ•ˆæœ |
|:-----|:-----|:-----|
| **Persistent Kernel** | å•æ¬¡ launch æ‰§è¡Œå¤šä»»åŠ¡ | 16x åŠ é€Ÿ |
| **GPU Atomics** | PTX æŒ‡ä»¤å®ç° SM é—´åŒæ­¥ | 1-2Î¼s å»¶è¿Ÿ |
| **TaskGraph API** | ç»†ç²’åº¦ä»»åŠ¡ä¾èµ–æ§åˆ¶ | è‡ªåŠ¨è°ƒåº¦ |
| **Compute-Comm Overlap** | è®¡ç®—ä¸é€šä¿¡å¹¶è¡Œæ‰§è¡Œ | 1.27x åŠ é€Ÿ |
| **Multi-GPU NCCL** | å¼‚æ­¥é€šä¿¡åŸè¯­ | 4+ GPU æ”¯æŒ |

## ğŸ—ï¸ æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User API                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TaskGraph  â”‚  â”‚   linear()  â”‚  â”‚  overlap().auto()   â”‚   â”‚
â”‚  â”‚   compile   â”‚  â”‚   matmul()  â”‚  â”‚                     â”‚   â”‚
â”‚  â”‚   execute   â”‚  â”‚   custom()  â”‚  â”‚                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Task Scheduler                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Dependency Inference  â†’  Wave Grouping  â†’  Execution   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       CUDA Runtime                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Worker CTA  â”‚  â”‚ Scheduler CTAâ”‚  â”‚   GPU Atomics    â”‚   â”‚
â”‚  â”‚  (Compute)   â”‚  â”‚   (Dispatch) â”‚  â”‚  (Sync Prims)    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Communication Backend                      â”‚
â”‚           NCCL (default)    â”‚    NVSHMEM (optional)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
git clone https://github.com/your-org/maco.git
cd maco
python3 maco/csrc/setup_sm.py build_ext --inplace
```

**ç¯å¢ƒè¦æ±‚**: Python 3.8+ / PyTorch 2.0+ / CUDA 11.0+ / GPU sm_70+ (V100/A100/H100)

### åŸºç¡€ç”¨æ³•

```python
import torch
from maco import TaskGraph

# åˆ›å»ºä»»åŠ¡å›¾
graph = TaskGraph(num_workers=8)

# å®šä¹‰è®¡ç®—ä»»åŠ¡
x = torch.randn(32, 512, device="cuda")
w1 = torch.randn(1024, 512, device="cuda")
w2 = torch.randn(512, 1024, device="cuda")

t1 = graph.linear(x, w1, name="proj_up")
t2 = graph.linear(t1.output, w2, name="proj_down")

# ç¼–è¯‘å¹¶æ‰§è¡Œ
graph.compile()
graph.execute()

print(t2.output.shape)  # torch.Size([32, 512])
```

### è®¡ç®—-é€šä¿¡é‡å 

```python
# è®¡ç®—ä»»åŠ¡
compute_tasks = []
h = x
for i, w in enumerate(weights):
    t = graph.linear(h, w, name=f"layer_{i}")
    compute_tasks.append(t)
    h = t.output

# é€šä¿¡ä»»åŠ¡
comm_task = graph.allreduce(gradient, name="sync")

# æ ‡è®°é‡å å¹¶è‡ªåŠ¨åˆ†é… wave
graph.overlap(compute_tasks, [comm_task]).auto_waves()

graph.compile()
graph.execute()
```

## ğŸ“Š æ€§èƒ½

åœ¨ NVIDIA L20 GPU ä¸Šæµ‹è¯•ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SM Scheduling Performance                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Workers    â”‚  Throughput     â”‚      Scaling          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      1       â”‚    4.18 GB/s    â”‚        1.0x           â”‚
â”‚      4       â”‚   15.12 GB/s    â”‚        3.6x           â”‚
â”‚      8       â”‚   29.81 GB/s    â”‚        7.1x           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Kernel Launch Overhead:     16.14x speedup
Compute-Comm Overlap:       92.2% efficiency
```

## ğŸ§ª æµ‹è¯•

```bash
# å•å…ƒæµ‹è¯• (å• GPU)
pytest tests/ -v

# å¤š GPU æµ‹è¯• (4x GPU)
torchrun --nproc_per_node=4 -m pytest tests/test_comm.py -v
torchrun --nproc_per_node=4 -m pytest tests/test_overlap.py -v

# æ€§èƒ½éªŒè¯
torchrun --nproc_per_node=4 python examples/test_real_overlap.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
maco/
â”œâ”€â”€ maco/
â”‚   â”œâ”€â”€ task_graph/              # Python API
â”‚   â”‚   â”œâ”€â”€ __init__.py          # TaskGraph, TaskNode, TaskSchedule
â”‚   â”‚   â”œâ”€â”€ runtime.py           # StreamRuntime
â”‚   â”‚   â”œâ”€â”€ overlap_scheduler.py # OverlapScheduler, OverlapRuntime
â”‚   â”‚   â”œâ”€â”€ exceptions.py        # Custom exceptions
â”‚   â”‚   â””â”€â”€ validation.py        # Input validation
â”‚   â”œâ”€â”€ comm/                    # Multi-GPU Communication (Phase 3)
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Module exports
â”‚   â”‚   â”œâ”€â”€ process_group.py     # ProcessGroupManager
â”‚   â”‚   â””â”€â”€ nccl_ops.py          # Async NCCL operations
â”‚   â”œâ”€â”€ sync/                    # Synchronization Primitives (Phase 3)
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Module exports
â”‚   â”‚   â”œâ”€â”€ signal_wait.py       # Signal-Wait, OverlapContext
â”‚   â”‚   â””â”€â”€ stream_manager.py    # StreamManager
â”‚   â””â”€â”€ csrc/                    # CUDA Core
â”‚       â”œâ”€â”€ maco_kernel.cu       # Persistent Kernel
â”‚       â”œâ”€â”€ maco_worker.cuh      # Worker CTA
â”‚       â”œâ”€â”€ maco_scheduler.cuh   # Scheduler CTA
â”‚       â””â”€â”€ maco_atoms.cuh       # GPU Atomics (PTX)
â”œâ”€â”€ tests/                       # Unit tests (55+ tests)
â”œâ”€â”€ examples/                    # Example scripts
â””â”€â”€ docs/                        # Documentation
```

## ğŸ—ºï¸ Roadmap

- [x] **Phase 1**: CUDA Core (GPU Atomics, Persistent Kernel)
- [x] **Phase 2**: TaskGraph API + Validation + Tests
- [x] **Phase 3**: Multi-GPU Support (NCCL, Signal-Wait, Compute-Comm Overlap)
- [ ] **Phase 4**: Model Integration (self-forcing, vLLM)

## ğŸ“š Documentation

- [Architecture Design](docs/architecture.md)
- [Technical Internals](docs/mirage_learnings.md)

## ğŸ™ Acknowledgments

SM scheduling techniques learned from [Mirage](https://github.com/mirage-project/mirage).

## ğŸ“„ License

MIT License
