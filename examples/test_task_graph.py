#!/usr/bin/env python3
"""
TaskGraph API æµ‹è¯•

æµ‹è¯• MACO Phase 2 çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. TaskGraph åˆ›å»ºå’Œç¼–è¯‘
2. ä»»åŠ¡ä¾èµ–æ¨æ–­
3. Wave Grouping
4. Stream æ¨¡å¼æ‰§è¡Œï¼ˆå›é€€ï¼‰

è¿è¡Œæ–¹å¼:
    cd /mini_mirage/maco
    CUDA_VISIBLE_DEVICES=1 python3 examples/test_task_graph.py
"""

import sys
import os
import time

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch


def test_basic_task_graph():
    """æµ‹è¯•åŸºæœ¬çš„ TaskGraph åˆ›å»º"""
    print("=" * 60)
    print("Test 1: Basic TaskGraph Creation")
    print("=" * 60)

    from maco import TaskGraph, TaskType

    # åˆ›å»ºä»»åŠ¡å›¾
    graph = TaskGraph(num_workers=4)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(32, 512, device="cuda")
    w1 = torch.randn(1024, 512, device="cuda")
    w2 = torch.randn(512, 1024, device="cuda")

    # æ·»åŠ ä»»åŠ¡
    t1 = graph.linear(x, w1, name="linear1")
    print(f"Created task: {t1.name}, type={t1.task_type.name}")

    t2 = graph.linear(t1.output, w2, name="linear2")
    print(f"Created task: {t2.name}, type={t2.task_type.name}")

    # æ£€æŸ¥ä¾èµ–æ¨æ–­
    graph._infer_dependencies()
    print(f"\nt2 depends on: {[d.name for d in t2.depends_on]}")

    print(f"\n{graph.summary()}")
    print("\nâœ“ Basic TaskGraph creation passed!")
    return True


def test_dependency_inference():
    """æµ‹è¯•ä¾èµ–æ¨æ–­"""
    print("\n" + "=" * 60)
    print("Test 2: Dependency Inference")
    print("=" * 60)

    from maco import TaskGraph

    graph = TaskGraph(num_workers=4)

    # åˆ›å»ºé“¾å¼ä»»åŠ¡
    x = torch.randn(32, 256, device="cuda")
    w1 = torch.randn(512, 256, device="cuda")
    w2 = torch.randn(256, 512, device="cuda")
    w3 = torch.randn(128, 256, device="cuda")

    t1 = graph.linear(x, w1, name="linear1")
    t2 = graph.linear(t1.output, w2, name="linear2")
    t3 = graph.linear(t2.output, w3, name="linear3")

    # æ¨æ–­ä¾èµ–
    graph._infer_dependencies()

    # éªŒè¯ä¾èµ–é“¾
    assert len(t1.depends_on) == 0, "t1 should have no dependencies"
    assert t1 in t2.depends_on, "t2 should depend on t1"
    assert t2 in t3.depends_on, "t3 should depend on t2"

    print("Dependency chain:")
    print(f"  t1 ({t1.name}): depends on {[d.name for d in t1.depends_on]}")
    print(f"  t2 ({t2.name}): depends on {[d.name for d in t2.depends_on]}")
    print(f"  t3 ({t3.name}): depends on {[d.name for d in t3.depends_on]}")

    print("\nâœ“ Dependency inference passed!")
    return True


def test_wave_grouping():
    """æµ‹è¯• Wave Grouping"""
    print("\n" + "=" * 60)
    print("Test 3: Wave Grouping")
    print("=" * 60)

    from maco import TaskGraph

    graph = TaskGraph(num_workers=4)

    # åˆ›å»ºå¤šä¸ªè®¡ç®—ä»»åŠ¡
    x = torch.randn(32, 256, device="cuda")
    tasks = []

    for i in range(8):
        w = torch.randn(256, 256, device="cuda")
        t = graph.linear(x, w, name=f"linear_{i}")
        tasks.append(t)
        x = t.output

    # åˆ›å»ºé€šä¿¡ä»»åŠ¡
    comm_tensor = torch.randn(32, 256, device="cuda")
    comm_task = graph.allreduce(comm_tensor, name="allreduce")

    # æ ‡è®°é‡å å¹¶è‡ªåŠ¨åˆ†ç»„
    group = graph.overlap(tasks, [comm_task])
    group.auto_waves()

    print(f"Total compute tasks: {len(tasks)}")
    print(f"Auto-detected waves: {group.num_waves}")
    print(f"Wave assignments:")
    for t in tasks:
        print(f"  {t.name}: wave {t._wave_id}")

    print("\nâœ“ Wave grouping passed!")
    return True


def test_task_schedule():
    """æµ‹è¯•ä»»åŠ¡è°ƒåº¦ç”Ÿæˆ"""
    print("\n" + "=" * 60)
    print("Test 4: Task Schedule Generation")
    print("=" * 60)

    from maco import TaskGraph, TaskSchedule

    graph = TaskGraph(num_workers=4)

    # åˆ›å»º DAG ç»“æ„çš„ä»»åŠ¡
    #     t1
    #    /  \
    #   t2   t3
    #    \  /
    #     t4

    x = torch.randn(32, 256, device="cuda")
    w1 = torch.randn(256, 256, device="cuda")
    w2 = torch.randn(256, 256, device="cuda")
    w3 = torch.randn(256, 256, device="cuda")
    w4 = torch.randn(128, 256, device="cuda")

    t1 = graph.linear(x, w1, name="t1")
    t2 = graph.linear(t1.output, w2, name="t2")
    t3 = graph.linear(t1.output, w3, name="t3")

    # t4 ä¾èµ– t2 å’Œ t3
    t4 = graph.linear(t2.output, w4, name="t4")
    t4.add_dependency(t3)

    # ç¼–è¯‘ç”Ÿæˆè°ƒåº¦
    graph.compile()

    print(f"Execution waves: {len(graph._schedule.waves)}")
    for i, wave in enumerate(graph._schedule.waves):
        names = [n.name for n in wave]
        print(f"  Wave {i}: {names}")

    # éªŒè¯æ‹“æ‰‘é¡ºåº
    wave_indices = {}
    for i, wave in enumerate(graph._schedule.waves):
        for node in wave:
            wave_indices[node.name] = i

    assert wave_indices["t1"] < wave_indices["t2"], "t1 should be before t2"
    assert wave_indices["t1"] < wave_indices["t3"], "t1 should be before t3"
    assert wave_indices["t2"] < wave_indices["t4"], "t2 should be before t4"
    assert wave_indices["t3"] < wave_indices["t4"], "t3 should be before t4"

    print("\nâœ“ Task schedule generation passed!")
    return True


def test_stream_execution():
    """æµ‹è¯• Stream æ¨¡å¼æ‰§è¡Œ"""
    print("\n" + "=" * 60)
    print("Test 5: Stream Mode Execution")
    print("=" * 60)

    from maco import TaskGraph

    graph = TaskGraph(num_workers=4)

    # åˆ›å»ºç®€å•çš„è®¡ç®—é“¾
    x = torch.randn(32, 512, device="cuda")
    w1 = torch.randn(1024, 512, device="cuda")
    w2 = torch.randn(512, 1024, device="cuda")

    t1 = graph.linear(x, w1, name="linear1")
    t2 = graph.linear(t1.output, w2, name="linear2")

    # ç¼–è¯‘
    graph.compile()

    # è®¡ç®—æœŸæœ›ç»“æœ
    expected = torch.nn.functional.linear(
        torch.nn.functional.linear(x, w1), w2
    )

    # æ‰§è¡Œ
    torch.cuda.synchronize()
    start = time.perf_counter()

    graph.execute()

    torch.cuda.synchronize()
    elapsed = (time.perf_counter() - start) * 1000

    # éªŒè¯ç»“æœ
    actual = t2.output
    diff = (expected - actual).abs().max().item()

    print(f"Execution time: {elapsed:.3f} ms")
    print(f"Max difference from expected: {diff:.6f}")

    assert diff < 1e-4, f"Result mismatch: {diff}"

    print("\nâœ“ Stream mode execution passed!")
    return True


def test_benchmark_stream():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("Test 6: Performance Benchmark")
    print("=" * 60)

    from maco import TaskGraph

    # æµ‹è¯•å‚æ•°
    batch_size = 64
    hidden_size = 1024
    num_layers = 4
    iterations = 10

    # å‡†å¤‡æ•°æ®
    x = torch.randn(batch_size, hidden_size, device="cuda")
    weights = [
        torch.randn(hidden_size, hidden_size, device="cuda")
        for _ in range(num_layers)
    ]

    # 1. PyTorch åŸºçº¿
    torch.cuda.synchronize()
    start = time.perf_counter()

    for _ in range(iterations):
        h = x
        for w in weights:
            h = torch.nn.functional.linear(h, w)
        torch.cuda.synchronize()

    baseline_time = (time.perf_counter() - start) / iterations * 1000

    # 2. TaskGraph æ‰§è¡Œ
    graph = TaskGraph(num_workers=4)

    h = x
    for i, w in enumerate(weights):
        task = graph.linear(h, w, name=f"layer_{i}")
        h = task.output

    graph.compile()

    # Warmup
    for _ in range(3):
        graph.execute()
    torch.cuda.synchronize()

    start = time.perf_counter()
    for _ in range(iterations):
        graph.execute()
    torch.cuda.synchronize()

    taskgraph_time = (time.perf_counter() - start) / iterations * 1000

    print(f"Batch size: {batch_size}, Hidden: {hidden_size}, Layers: {num_layers}")
    print(f"PyTorch baseline: {baseline_time:.3f} ms")
    print(f"TaskGraph (stream): {taskgraph_time:.3f} ms")
    print(f"Ratio: {taskgraph_time / baseline_time:.2f}x")

    print("\nâœ“ Benchmark completed!")
    return True


def test_custom_task():
    """æµ‹è¯•è‡ªå®šä¹‰ä»»åŠ¡"""
    print("\n" + "=" * 60)
    print("Test 7: Custom Task")
    print("=" * 60)

    from maco import TaskGraph

    graph = TaskGraph(num_workers=4)

    x = torch.randn(32, 256, device="cuda")
    output = torch.empty(32, 256, device="cuda")

    # è‡ªå®šä¹‰å‡½æ•°ï¼šReLU + Scale
    def custom_fn(inp):
        return torch.relu(inp) * 2.0

    t = graph.custom(
        fn=custom_fn,
        inputs=[x],
        outputs=[output],
        name="custom_relu_scale",
    )

    graph.compile()
    graph.execute()

    # éªŒè¯
    expected = torch.relu(x) * 2.0
    diff = (expected - output).abs().max().item()

    print(f"Custom task result diff: {diff:.6f}")
    assert diff < 1e-5, f"Custom task result mismatch: {diff}"

    print("\nâœ“ Custom task passed!")
    return True


def main():
    print("=" * 60)
    print("MACO TaskGraph API Test Suite")
    print("=" * 60)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    if device.type != "cuda":
        print("CUDA not available, some tests may fail.")

    if device.type == "cuda":
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"CUDA Version: {torch.version.cuda}")

    # è¿è¡Œæµ‹è¯•
    tests = [
        test_basic_task_graph,
        test_dependency_inference,
        test_wave_grouping,
        test_task_schedule,
        test_stream_execution,
        test_custom_task,
        test_benchmark_stream,
    ]

    passed = 0
    failed = 0

    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"\nâœ— {test.__name__} failed with exception: {e}")
            import traceback
            traceback.print_exc()
            failed += 1

    print("\n" + "=" * 60)
    print(f"Results: {passed} passed, {failed} failed")
    print("=" * 60)

    if failed == 0:
        print("\nğŸ‰ All tests passed! TaskGraph API is ready.")
    else:
        print(f"\nâš ï¸ {failed} tests failed.")


if __name__ == "__main__":
    main()
